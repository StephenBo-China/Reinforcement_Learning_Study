{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0\n",
    "- The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "- Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    state_space_n = env.observation_space.n\n",
    "    action_space_n = env.action_space.n\n",
    "    print(state_space_n)\n",
    "    print(action_space_n)\n",
    "    return env, state_space_n, action_space_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 策略迭代[基于贪婪策略]\n",
    "个体在处于任一状态时,将比较所有可能后续状态的价值，从中选择最大价值的状态，再选择能到该状态的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "env, state_space_n, action_space_n = get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_flag(value_table1, value_table2, threshold = 1e-1):\n",
    "    if np.sum(np.fabs(value_table1-value_table2)) < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代函数：\n",
    "$$\n",
    "V_{k+1}(s) = \\sum_{a \\in A}\\pi(a|s)(R_{s}^{a} + \\gamma \\sum_{s^{'}\\in S}P^{a}_{ss^{'}}V_{k}(s^{'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = np.zeros(16)\n",
    "pi = 0.25\n",
    "gamma = 1.0\n",
    "iteration_num = 10000\n",
    "value_table.reshape([4, 4])\n",
    "action_list = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_state(value_table, actions, state):\n",
    "    max_value = -1000\n",
    "    rst = 0\n",
    "    for action in actions:\n",
    "        for next_states in env.P[state][action]:\n",
    "            pi, next_state, reward, done = next_states\n",
    "            if value_table[next_state] > max_value:\n",
    "                max_value = value_table[next_state]\n",
    "                rst = next_state\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(iteration_num):\n",
    "    value_table_tmp = np.copy(value_table)\n",
    "    for s in range(state_space_n):\n",
    "        flag = False\n",
    "        if s not in [0, 15]:\n",
    "            value_s = []\n",
    "            next_state_max = get_max_state(value_table_tmp, action_list, s)\n",
    "            #print(next_state_max)\n",
    "            for a in range(action_space_n):\n",
    "                q_tmp = []\n",
    "                rewards = 0\n",
    "                for next_state_info in env.P[s][a]:\n",
    "                    P, next_state, reward, done =  next_state_info\n",
    "                    if reward == 0:\n",
    "                        reward = -1\n",
    "                    if reward == 1:\n",
    "                        reward = 0\n",
    "                    flag = done\n",
    "                    #print(next_state_info)\n",
    "                    if next_state == next_state_max:#选择能到达最大value状态的随机一个action\n",
    "                        rewards += reward * P\n",
    "                        q_tmp.append(P * value_table_tmp[next_state])\n",
    "                        break\n",
    "                #print(\"{}*({} + {} * {})\".format(pi, rewards, gamma,np.sum(q_tmp)))\n",
    "                value_s.append(pi * (rewards + gamma * np.sum(q_tmp)))\n",
    "            value_table[s] = np.sum(value_s)\n",
    "#         if flag:\n",
    "#             break\n",
    "    value_table_print = np.copy(value_table)\n",
    "    value_table_print = value_table_print.reshape([4, 4])\n",
    "    #print(\"[Time k={}]\".format(k))\n",
    "    #print(value_table_print)\n",
    "    if convergence_flag(value_table_tmp, value_table):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000e+00, -2.50000e-01, -3.12500e-01, -3.28125e-01],\n",
       "       [-2.50000e-01, -9.91000e+02, -3.12500e-01, -9.91000e+02],\n",
       "       [-3.12500e-01, -3.12500e-01, -3.12500e-01, -9.91000e+02],\n",
       "       [-9.91000e+02, -3.12500e-01,  0.00000e+00,  0.00000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table.reshape([4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_state(state, action_space_n):\n",
    "    rst = set()\n",
    "    for a in range(action_space_n):\n",
    "        for next_states in env.P[state][a]:\n",
    "            P, next_state, reward, done = next_states \n",
    "            rst.add(next_state)\n",
    "    return rst\n",
    "def get_max_state(state_now, states, value_table):\n",
    "    rst = 0\n",
    "    max_value = -0xffffff\n",
    "    for state in states:\n",
    "        if value_table[state] > max_value and state != state_now:\n",
    "            max_value = value_table[state]\n",
    "            rst = state\n",
    "    return rst\n",
    "def get_state_action(state_now, state_next, action_space_n):\n",
    "    rst = set()\n",
    "    for a in range(action_space_n):\n",
    "        for next_states in env.P[state_now][a]:\n",
    "            P, next_state, reward, done = next_states\n",
    "            if next_state == state_next:\n",
    "                rst.add(a)\n",
    "    return list(rst)\n",
    "def get_action(state, value_table, action_space_n):\n",
    "    possible_states = get_possible_state(state, action_space_n)\n",
    "    max_next_state = get_max_state(state, possible_states, value_table)\n",
    "    action_list = get_state_action(state, max_next_state, action_space_n)\n",
    "    return action_list[random.randint(0,len(action_list) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_success_rate(env, value_table, action_space_n, start_state = 0):\n",
    "    success = 0\n",
    "    success_temp = []\n",
    "    for i in range(10000):\n",
    "        done = False\n",
    "        state_now = start_state\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            action = get_action(state_now, value_table, action_space_n)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if next_state == 15:\n",
    "                success += 1\n",
    "            state_now = next_state\n",
    "    return success / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_success_rate(env, value_table, action_space_n, start_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, value_table, action_space_n, start_state = 0):\n",
    "    done = False\n",
    "    state_now = start_state\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        action = get_action(state_now, value_table, action_space_n)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state_now = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "test(env, value_table, action_space_n, start_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
