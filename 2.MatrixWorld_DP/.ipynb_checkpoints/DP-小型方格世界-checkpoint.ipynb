{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态规划寻找最优策略\n",
    "## 策略评估、策略迭代、价值迭代\n",
    "- 预测：\n",
    "已知一个马尔科夫决策过程$MDP<S,A,P,R,\\gamma>$和一个策略$\\pi$，或者是给定一个马尔科夫奖励过程$MRP<S,P_{\\pi},R_{\\pi},\\gamma>$，求解基于该策略的价值函数$v_{\\pi}$\n",
    "- 控制：\n",
    "已知一个马尔科夫决策过程$MDP<S,A,P,R,\\gamma>$，求解最优价值函数$v_{*}$和最优策略$\\pi_{*}$\n",
    "## 1. 定义小型方格世界环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self):\n",
    "        self.action = np.arange(0,4,1)\n",
    "        self.state = np.arange(0,16,1)\n",
    "        self.action_space_n = len(self.action)\n",
    "        self.state_space_n = len(self.state)\n",
    "        self.P = self.get_P()\n",
    "        \n",
    "    def get_P(self):\n",
    "        rst = dict()\n",
    "        for s in self.state:\n",
    "            action_dict = dict()\n",
    "            for a in self.action:\n",
    "                next_state = self.get_next_state(a, s)\n",
    "                reward = -1\n",
    "                flag = False\n",
    "                if next_state == 15:\n",
    "                    flag = True\n",
    "                    reward = 0  \n",
    "                action_dict[a] = [[self.get_next_state(a, s), reward, 1.0, flag]]\n",
    "            rst[s] = action_dict\n",
    "        return rst\n",
    "                \n",
    "    def get_next_state(self, action, state):\n",
    "        if action == 0:#up\n",
    "            return (lambda x: x if x - 4 < 0 else x - 4)(state)\n",
    "        elif action == 1:#down\n",
    "            return (lambda x: x if x + 4 > 15 else x + 4)(state)\n",
    "        elif action == 2:#left\n",
    "            return (lambda x: x if x % 4 == 0 else x - 1)(state)\n",
    "        else:\n",
    "            return (lambda x: x if x in [3, 7, 11, 15] else x + 1)(state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ENV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [[0, -1, 1.0, False]],\n",
       "  1: [[4, -1, 1.0, False]],\n",
       "  2: [[0, -1, 1.0, False]],\n",
       "  3: [[1, -1, 1.0, False]]},\n",
       " 1: {0: [[1, -1, 1.0, False]],\n",
       "  1: [[5, -1, 1.0, False]],\n",
       "  2: [[0, -1, 1.0, False]],\n",
       "  3: [[2, -1, 1.0, False]]},\n",
       " 2: {0: [[2, -1, 1.0, False]],\n",
       "  1: [[6, -1, 1.0, False]],\n",
       "  2: [[1, -1, 1.0, False]],\n",
       "  3: [[3, -1, 1.0, False]]},\n",
       " 3: {0: [[3, -1, 1.0, False]],\n",
       "  1: [[7, -1, 1.0, False]],\n",
       "  2: [[2, -1, 1.0, False]],\n",
       "  3: [[3, -1, 1.0, False]]},\n",
       " 4: {0: [[0, -1, 1.0, False]],\n",
       "  1: [[8, -1, 1.0, False]],\n",
       "  2: [[4, -1, 1.0, False]],\n",
       "  3: [[5, -1, 1.0, False]]},\n",
       " 5: {0: [[1, -1, 1.0, False]],\n",
       "  1: [[9, -1, 1.0, False]],\n",
       "  2: [[4, -1, 1.0, False]],\n",
       "  3: [[6, -1, 1.0, False]]},\n",
       " 6: {0: [[2, -1, 1.0, False]],\n",
       "  1: [[10, -1, 1.0, False]],\n",
       "  2: [[5, -1, 1.0, False]],\n",
       "  3: [[7, -1, 1.0, False]]},\n",
       " 7: {0: [[3, -1, 1.0, False]],\n",
       "  1: [[11, -1, 1.0, False]],\n",
       "  2: [[6, -1, 1.0, False]],\n",
       "  3: [[7, -1, 1.0, False]]},\n",
       " 8: {0: [[4, -1, 1.0, False]],\n",
       "  1: [[12, -1, 1.0, False]],\n",
       "  2: [[8, -1, 1.0, False]],\n",
       "  3: [[9, -1, 1.0, False]]},\n",
       " 9: {0: [[5, -1, 1.0, False]],\n",
       "  1: [[13, -1, 1.0, False]],\n",
       "  2: [[8, -1, 1.0, False]],\n",
       "  3: [[10, -1, 1.0, False]]},\n",
       " 10: {0: [[6, -1, 1.0, False]],\n",
       "  1: [[14, -1, 1.0, False]],\n",
       "  2: [[9, -1, 1.0, False]],\n",
       "  3: [[11, -1, 1.0, False]]},\n",
       " 11: {0: [[7, -1, 1.0, False]],\n",
       "  1: [[15, 0, 1.0, True]],\n",
       "  2: [[10, -1, 1.0, False]],\n",
       "  3: [[11, -1, 1.0, False]]},\n",
       " 12: {0: [[8, -1, 1.0, False]],\n",
       "  1: [[12, -1, 1.0, False]],\n",
       "  2: [[12, -1, 1.0, False]],\n",
       "  3: [[13, -1, 1.0, False]]},\n",
       " 13: {0: [[9, -1, 1.0, False]],\n",
       "  1: [[13, -1, 1.0, False]],\n",
       "  2: [[12, -1, 1.0, False]],\n",
       "  3: [[14, -1, 1.0, False]]},\n",
       " 14: {0: [[10, -1, 1.0, False]],\n",
       "  1: [[14, -1, 1.0, False]],\n",
       "  2: [[13, -1, 1.0, False]],\n",
       "  3: [[15, 0, 1.0, True]]},\n",
       " 15: {0: [[11, -1, 1.0, False]],\n",
       "  1: [[15, 0, 1.0, True]],\n",
       "  2: [[14, -1, 1.0, False]],\n",
       "  3: [[15, 0, 1.0, True]]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(env.state_space_n)\n",
    "print(env.action_space_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 策略评估[小型方格世界]\n",
    "### 基于均一随机策略\n",
    "$$\n",
    "V_{k+1}(s) = \\sum_{a \\in A}\\pi (a|s) (R_{s}^{a} + \\gamma \\sum_{s^{'} \\in S} P^{a}_{s^{'}}v_{k}(s^{'}))\n",
    "$$\n",
    "迭代过程中价值函数更新方式:https://subaochen.github.io/reinforcement%20learning/2019/06/16/policy-evaluation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_flag(value_table1, value_table2, threshold = 1e-2):\n",
    "    if np.sum(np.fabs(value_table1-value_table2)) < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = np.zeros(16)\n",
    "pi = 0.25\n",
    "gamma = 1.0\n",
    "value_table\n",
    "iteration_num = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time k=0]\n",
      "[[ 0.   -1.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.   -1.  ]\n",
      " [-1.   -1.   -1.   -0.75]\n",
      " [-1.   -1.   -0.75  0.  ]]\n",
      "[Time k=1]\n",
      "[[ 0.     -1.75   -2.     -2.    ]\n",
      " [-1.75   -2.     -2.     -1.9375]\n",
      " [-2.     -2.     -1.875  -1.4375]\n",
      " [-2.     -1.9375 -1.4375  0.    ]]\n",
      "[Time k=2]\n",
      "[[ 0.       -2.4375   -2.9375   -2.984375]\n",
      " [-2.4375   -2.875    -2.953125 -2.84375 ]\n",
      " [-2.9375   -2.953125 -2.71875  -2.0625  ]\n",
      " [-2.984375 -2.84375  -2.0625    0.      ]]\n",
      "[Time k=3]\n",
      "[[ 0.        -3.0625    -3.828125  -3.9375   ]\n",
      " [-3.0625    -3.6953125 -3.84375   -3.7109375]\n",
      " [-3.828125  -3.84375   -3.5078125 -2.65625  ]\n",
      " [-3.9375    -3.7109375 -2.65625    0.       ]]\n",
      "[Time k=4]\n",
      "[[ 0.         -3.64648438 -4.66796875 -4.85351562]\n",
      " [-3.64648438 -4.453125   -4.68554688 -4.53710938]\n",
      " [-4.66796875 -4.68554688 -4.25       -3.21875   ]\n",
      " [-4.85351562 -4.53710938 -3.21875     0.        ]]\n",
      "[Time k=5]\n",
      "[[ 0.         -4.19189453 -5.46337891 -5.72802734]\n",
      " [-4.19189453 -5.16601562 -5.47705078 -5.32373047]\n",
      " [-5.46337891 -5.47705078 -4.95214844 -3.75146484]\n",
      " [-5.72802734 -5.32373047 -3.75146484  0.        ]]\n",
      "[Time k=6]\n",
      "[[ 0.         -4.70532227 -6.21508789 -6.56079102]\n",
      " [-4.70532227 -5.83447266 -6.22631836 -6.07006836]\n",
      " [-6.21508789 -6.22631836 -5.61425781 -4.25683594]\n",
      " [-6.56079102 -6.07006836 -4.25683594  0.        ]]\n",
      "[Time k=7]\n",
      "[[ 0.         -5.1887207  -6.92687988 -7.35168457]\n",
      " [-5.1887207  -6.46582031 -6.93347168 -6.77850342]\n",
      " [-6.92687988 -6.93347168 -6.24157715 -4.73529053]\n",
      " [-7.35168457 -6.77850342 -4.73529053  0.        ]]\n",
      "[Time k=8]\n",
      "[[ 0.         -5.64535522 -7.60018921 -8.10218811]\n",
      " [-5.64535522 -7.06109619 -7.60319519 -7.44973755]\n",
      " [-7.60018921 -7.60319519 -6.8343811  -5.18884277]\n",
      " [-8.10218811 -7.44973755 -5.18884277  0.        ]]\n",
      "[Time k=9]\n",
      "[[ 0.         -6.07666016 -8.23773193 -8.81357574]\n",
      " [-6.07666016 -7.62427521 -8.23635101 -8.08599091]\n",
      " [-8.23773193 -8.23635101 -7.39601898 -5.61824036]\n",
      " [-8.81357574 -8.08599091 -5.61824036  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for k in range(iteration_num):\n",
    "    value_table_tmp = np.copy(value_table)\n",
    "    for s in range(env.state_space_n):\n",
    "        if s not in [0, 15]:\n",
    "            value_s = []\n",
    "            for a in env.action:\n",
    "                q_tmp = []\n",
    "                rewards = 0\n",
    "                for next_state_info in env.P[s][a]:\n",
    "                    next_state, reward, P, done =  next_state_info\n",
    "                    rewards += reward * P\n",
    "                    q_tmp.append(P * value_table_tmp[next_state])\n",
    "                #print(\"{}*({} + {} * {} * {})\".format(pi, rewards, gamma, P, value_table[next_state]))\n",
    "                value_s.append(pi * (rewards+ gamma * np.sum(q_tmp)))\n",
    "            value_table[s] = np.sum(value_s)\n",
    "    value_table_print = np.copy(value_table)\n",
    "    value_table_print = value_table_print.reshape([4, 4])\n",
    "    if k < 10:\n",
    "        print(\"[Time k={}]\".format(k))\n",
    "        print(value_table_print)\n",
    "    if convergence_flag(value_table_tmp, value_table):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -13.72157009, -19.56329161, -21.48474559],\n",
       "       [-13.72157009, -17.6033761 , -19.48645972, -19.40944546],\n",
       "       [-19.56329161, -19.48645972, -17.37260687, -13.26003163],\n",
       "       [-21.48474559, -19.40944546, -13.26003163,   0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table.reshape([4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 策略迭代[小型方格世界]\n",
    "我们在策略评估过程中使用贪婪策略更新我们的策略$\\pi$, 即$\\pi^{'} = greedy(\\pi)$\n",
    "- 问题：为什么取$\\pi^{'} = greedy(\\pi)$收敛可到达最优解证明：</br>\n",
    "\n",
    "见：笔记 p35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(action, state):\n",
    "    if action == 0:#up\n",
    "        return (lambda x: x if x - 4 < 0 else x - 4)(state)\n",
    "    elif action == 1:#down\n",
    "        return (lambda x: x if x + 4 > 15 else x + 4)(state)\n",
    "    elif action == 2:#left\n",
    "        return (lambda x: x if x % 4 == 0 else x - 1)(state)\n",
    "    else:\n",
    "        return (lambda x: x if x in [3, 7, 11, 15] else x + 1)(state)\n",
    "def get_max_state(value_table, actions, state):\n",
    "    max_value = -1000\n",
    "    rst = 0\n",
    "    for action in actions:\n",
    "        next_state = get_next_state(action, state)\n",
    "        if value_table[next_state] > max_value:\n",
    "            max_value = value_table[next_state]\n",
    "            rst = next_state\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = np.zeros(16)\n",
    "pi = 0.25\n",
    "gamma = 1.0\n",
    "value_table\n",
    "iteration_num = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time k=0]\n",
      "[[ 0.   -0.25 -0.25 -0.5 ]\n",
      " [-0.25 -0.25 -0.25 -0.25]\n",
      " [-0.25 -0.25 -0.25 -0.25]\n",
      " [-0.25 -0.25 -0.25  0.  ]]\n",
      "[Time k=1]\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.3125]\n",
      " [-0.3125 -0.3125 -0.3125  0.    ]\n",
      " [-0.3125 -0.3125  0.      0.    ]]\n",
      "[Time k=2]\n",
      "[[ 0.       -0.25     -0.3125   -0.65625 ]\n",
      " [-0.25     -0.3125   -0.328125 -0.25    ]\n",
      " [-0.3125   -0.328125 -0.25      0.      ]\n",
      " [-0.328125 -0.25     -0.25      0.      ]]\n",
      "[Time k=3]\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.25  ]\n",
      " [-0.3125 -0.3125 -0.25    0.    ]\n",
      " [-0.3125 -0.3125  0.      0.    ]]\n",
      "[Time k=4]\n",
      "[[ 0.       -0.25     -0.3125   -0.3125  ]\n",
      " [-0.25     -0.3125   -0.3125   -0.25    ]\n",
      " [-0.3125   -0.3125   -0.25      0.      ]\n",
      " [-0.328125 -0.25     -0.25      0.      ]]\n",
      "[Time k=5]\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.25  ]\n",
      " [-0.3125 -0.3125 -0.25    0.    ]\n",
      " [-0.3125 -0.3125  0.      0.    ]]\n",
      "[Time k=6]\n",
      "[[ 0.       -0.25     -0.3125   -0.3125  ]\n",
      " [-0.25     -0.3125   -0.3125   -0.25    ]\n",
      " [-0.3125   -0.3125   -0.25      0.      ]\n",
      " [-0.328125 -0.25     -0.25      0.      ]]\n",
      "[Time k=7]\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.25  ]\n",
      " [-0.3125 -0.3125 -0.25    0.    ]\n",
      " [-0.3125 -0.3125  0.      0.    ]]\n",
      "[Time k=8]\n",
      "[[ 0.       -0.25     -0.3125   -0.3125  ]\n",
      " [-0.25     -0.3125   -0.3125   -0.25    ]\n",
      " [-0.3125   -0.3125   -0.25      0.      ]\n",
      " [-0.328125 -0.25     -0.25      0.      ]]\n",
      "[Time k=9]\n",
      "[[ 0.     -0.25   -0.3125 -0.3125]\n",
      " [-0.25   -0.3125 -0.3125 -0.25  ]\n",
      " [-0.3125 -0.3125 -0.25    0.    ]\n",
      " [-0.3125 -0.3125  0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "for k in range(iteration_num):\n",
    "    value_table_tmp = np.copy(value_table)\n",
    "    for s in range(env.state_space_n):\n",
    "        if s not in [0, 15]:\n",
    "            value_s = []\n",
    "            next_state_max = get_max_state(value_table_tmp, env.action, s)\n",
    "            for a in env.action:\n",
    "                q_tmp = []\n",
    "                rewards = 0\n",
    "                for next_state_info in env.P[s][a]:\n",
    "                    next_state, reward, P, done =  next_state_info\n",
    "                    if next_state == next_state_max:#选择能到达最大value状态的随机一个action\n",
    "                        rewards += reward * 1\n",
    "                        q_tmp.append(1 * value_table_tmp[next_state])\n",
    "                        break\n",
    "                #print(\"{}*({} + {} * {} * {})\".format(pi, rewards, gamma, P, value_table[next_state]))\n",
    "                value_s.append(pi * (rewards + gamma * np.sum(q_tmp)))\n",
    "            value_table[s] = np.sum(value_s)\n",
    "    value_table_print = np.copy(value_table)\n",
    "    value_table_print = value_table_print.reshape([4, 4])\n",
    "    if k <10:\n",
    "        print(\"[Time k={}]\".format(k))\n",
    "        print(value_table_print)\n",
    "    if convergence_flag(value_table_tmp, value_table):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    , -0.25  , -0.3125, -0.3125],\n",
       "       [-0.25  , -0.3125, -0.3125, -0.25  ],\n",
       "       [-0.3125, -0.3125, -0.25  ,  0.    ],\n",
       "       [-0.3125, -0.3125,  0.    ,  0.    ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table.reshape([4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 价值迭代：\n",
    "$$\n",
    "v_{*}(s)=max_{a \\in A}(R_{s}^{a} + \\gamma \\sum_{s^{'} \\in S}P_{ss^{'}}^{a}v_{*}(s^{'}))\n",
    "$$\n",
    "- 求解时对于某一个state来说：\n",
    "\n",
    "迭代以下公式，对于某个state的value来说直到$v_{k+1}(s)$与$v_{k}(s)$的$\\delta$基本为0时得到state s下最优$v_{*}(s)$\n",
    "$$\n",
    "v_{k+1}(s)=max_{a \\in A}(R_{s}^{a} + \\gamma \\sum_{s^{'} \\in S}P_{ss^{'}}^{a}v_{k}(s^{'}))\n",
    "$$\n",
    "## (a)已知终态与终态价值根据如下公式迭代求解：\n",
    "从终态开始往前迭代使用DP求解之前状态的最优$v(s)$\n",
    "## 环境定义：[小型方格世界最短路径问题]\n",
    "从除$s_{0}$的任何位置出发以最短路径到达$s_{0}$\n",
    "\n",
    "终点为$s_{0}$,到达终点reward为0，其余state reward为-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self):\n",
    "        self.action = np.arange(0,4,1)\n",
    "        self.state = np.arange(0,16,1)\n",
    "        self.action_space_n = len(self.action)\n",
    "        self.state_space_n = len(self.state)\n",
    "        self.P = self.get_P()\n",
    "        \n",
    "    def get_P(self):\n",
    "        rst = dict()\n",
    "        for s in self.state:\n",
    "            action_dict = dict()\n",
    "            for a in self.action:\n",
    "                next_state = self.get_next_state(a, s)\n",
    "                reward = -1\n",
    "                flag = False\n",
    "                if next_state == 0:\n",
    "                    flag = True\n",
    "                    reward = 0  \n",
    "                action_dict[a] = [[self.get_next_state(a, s), reward, 1.0, flag]]\n",
    "            rst[s] = action_dict\n",
    "        return rst\n",
    "                \n",
    "    def get_next_state(self, action, state):\n",
    "        if action == 0:#up\n",
    "            return (lambda x: x if x - 4 < 0 else x - 4)(state)\n",
    "        elif action == 1:#down\n",
    "            return (lambda x: x if x + 4 > 15 else x + 4)(state)\n",
    "        elif action == 2:#left\n",
    "            return (lambda x: x if x % 4 == 0 else x - 1)(state)\n",
    "        else:\n",
    "            return (lambda x: x if x in [3, 7, 11, 15] else x + 1)(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [[0, 0, 1.0, True]],\n",
       "  1: [[4, -1, 1.0, False]],\n",
       "  2: [[0, 0, 1.0, True]],\n",
       "  3: [[1, -1, 1.0, False]]},\n",
       " 1: {0: [[1, -1, 1.0, False]],\n",
       "  1: [[5, -1, 1.0, False]],\n",
       "  2: [[0, 0, 1.0, True]],\n",
       "  3: [[2, -1, 1.0, False]]},\n",
       " 2: {0: [[2, -1, 1.0, False]],\n",
       "  1: [[6, -1, 1.0, False]],\n",
       "  2: [[1, -1, 1.0, False]],\n",
       "  3: [[3, -1, 1.0, False]]},\n",
       " 3: {0: [[3, -1, 1.0, False]],\n",
       "  1: [[7, -1, 1.0, False]],\n",
       "  2: [[2, -1, 1.0, False]],\n",
       "  3: [[3, -1, 1.0, False]]},\n",
       " 4: {0: [[0, 0, 1.0, True]],\n",
       "  1: [[8, -1, 1.0, False]],\n",
       "  2: [[4, -1, 1.0, False]],\n",
       "  3: [[5, -1, 1.0, False]]},\n",
       " 5: {0: [[1, -1, 1.0, False]],\n",
       "  1: [[9, -1, 1.0, False]],\n",
       "  2: [[4, -1, 1.0, False]],\n",
       "  3: [[6, -1, 1.0, False]]},\n",
       " 6: {0: [[2, -1, 1.0, False]],\n",
       "  1: [[10, -1, 1.0, False]],\n",
       "  2: [[5, -1, 1.0, False]],\n",
       "  3: [[7, -1, 1.0, False]]},\n",
       " 7: {0: [[3, -1, 1.0, False]],\n",
       "  1: [[11, -1, 1.0, False]],\n",
       "  2: [[6, -1, 1.0, False]],\n",
       "  3: [[7, -1, 1.0, False]]},\n",
       " 8: {0: [[4, -1, 1.0, False]],\n",
       "  1: [[12, -1, 1.0, False]],\n",
       "  2: [[8, -1, 1.0, False]],\n",
       "  3: [[9, -1, 1.0, False]]},\n",
       " 9: {0: [[5, -1, 1.0, False]],\n",
       "  1: [[13, -1, 1.0, False]],\n",
       "  2: [[8, -1, 1.0, False]],\n",
       "  3: [[10, -1, 1.0, False]]},\n",
       " 10: {0: [[6, -1, 1.0, False]],\n",
       "  1: [[14, -1, 1.0, False]],\n",
       "  2: [[9, -1, 1.0, False]],\n",
       "  3: [[11, -1, 1.0, False]]},\n",
       " 11: {0: [[7, -1, 1.0, False]],\n",
       "  1: [[15, -1, 1.0, False]],\n",
       "  2: [[10, -1, 1.0, False]],\n",
       "  3: [[11, -1, 1.0, False]]},\n",
       " 12: {0: [[8, -1, 1.0, False]],\n",
       "  1: [[12, -1, 1.0, False]],\n",
       "  2: [[12, -1, 1.0, False]],\n",
       "  3: [[13, -1, 1.0, False]]},\n",
       " 13: {0: [[9, -1, 1.0, False]],\n",
       "  1: [[13, -1, 1.0, False]],\n",
       "  2: [[12, -1, 1.0, False]],\n",
       "  3: [[14, -1, 1.0, False]]},\n",
       " 14: {0: [[10, -1, 1.0, False]],\n",
       "  1: [[14, -1, 1.0, False]],\n",
       "  2: [[13, -1, 1.0, False]],\n",
       "  3: [[15, -1, 1.0, False]]},\n",
       " 15: {0: [[11, -1, 1.0, False]],\n",
       "  1: [[15, -1, 1.0, False]],\n",
       "  2: [[14, -1, 1.0, False]],\n",
       "  3: [[15, -1, 1.0, False]]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ENV()\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0., -65535., -65535., -65535.],\n",
       "       [-65535., -65535., -65535., -65535.],\n",
       "       [-65535., -65535., -65535., -65535.],\n",
       "       [-65535., -65535., -65535., -65535.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table = np.zeros(16) + -0xffff\n",
    "value_table[0] = 0\n",
    "gamma = 1.0\n",
    "final_state = 0\n",
    "value_table.reshape([4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0., -65536., -65536., -65536.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_step_lookahead(state, value_table):\n",
    "    q_table = np.zeros(env.action_space_n)\n",
    "    for a in range(env.action_space_n):\n",
    "        for next_state, reward, prob, done in env.P[state][a]:\n",
    "            #print([prob, next_state, reward, done])\n",
    "            q_table[a] += prob * (reward + gamma * value_table[next_state])\n",
    "            #print(q_table)\n",
    "    return q_table\n",
    "def get_next_state(action, state):\n",
    "    if action == 0:#up\n",
    "        return (lambda x: x if x - 4 < 0 else x - 4)(state)\n",
    "    elif action == 1:#down\n",
    "        return (lambda x: x if x + 4 > 15 else x + 4)(state)\n",
    "    elif action == 2:#left\n",
    "        return (lambda x: x if x % 4 == 0 else x - 1)(state)\n",
    "    else:\n",
    "        return (lambda x: x if x in [3, 7, 11, 15] else x + 1)(state)\n",
    "one_step_lookahead(4, value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_processed_state = [final_state]\n",
    "def get_nearby_state(s):\n",
    "    rst = []\n",
    "    for action in env.action:\n",
    "        next_state = get_next_state (action, s)\n",
    "        if next_state not in already_processed_state:\n",
    "            rst.append(next_state)\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 1]:\n",
      "After update state:4\n",
      "[[     0. -65535. -65535. -65535.]\n",
      " [     0. -65535. -65535. -65535.]\n",
      " [-65535. -65535. -65535. -65535.]\n",
      " [-65535. -65535. -65535. -65535.]]\n",
      "After update state:1\n",
      "[[     0.      0. -65535. -65535.]\n",
      " [     0. -65535. -65535. -65535.]\n",
      " [-65535. -65535. -65535. -65535.]\n",
      " [-65535. -65535. -65535. -65535.]]\n",
      "After update state:5\n",
      "[[ 0.0000e+00  0.0000e+00 -6.5535e+04 -6.5535e+04]\n",
      " [ 0.0000e+00 -1.0000e+00 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:2\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -6.5535e+04]\n",
      " [ 0.0000e+00 -1.0000e+00 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:6\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -6.5535e+04]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:3\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:7\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:11\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -4.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -6.5535e+04]]\n",
      "After update state:15\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -4.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -5.0000e+00]]\n",
      "After update state:10\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -3.0000e+00 -4.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -6.5535e+04 -5.0000e+00]]\n",
      "After update state:14\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -3.0000e+00 -4.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -4.0000e+00 -5.0000e+00]]\n",
      "After update state:9\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -2.0000e+00 -3.0000e+00 -4.0000e+00]\n",
      " [-6.5535e+04 -6.5535e+04 -4.0000e+00 -5.0000e+00]]\n",
      "After update state:13\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-6.5535e+04 -2.0000e+00 -3.0000e+00 -4.0000e+00]\n",
      " [-6.5535e+04 -3.0000e+00 -4.0000e+00 -5.0000e+00]]\n",
      "After update state:8\n",
      "[[ 0.0000e+00  0.0000e+00 -1.0000e+00 -2.0000e+00]\n",
      " [ 0.0000e+00 -1.0000e+00 -2.0000e+00 -3.0000e+00]\n",
      " [-1.0000e+00 -2.0000e+00 -3.0000e+00 -4.0000e+00]\n",
      " [-6.5535e+04 -3.0000e+00 -4.0000e+00 -5.0000e+00]]\n",
      "After update state:12\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:4\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:1\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:5\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:2\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:6\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:3\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:7\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:11\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:10\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:14\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:9\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:13\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:8\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "After update state:12\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Final Value Table] is:\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n"
     ]
    }
   ],
   "source": [
    "episode = 1\n",
    "while True:\n",
    "    print(\"[Episode {}]:\".format(episode))\n",
    "    process_state_queue = [final_state]\n",
    "    while len(process_state_queue) != 0:\n",
    "        max_value_next_state = process_state_queue.pop(0)\n",
    "        nearby_states = get_nearby_state(max_value_next_state)\n",
    "        for state in nearby_states:\n",
    "            q_value = one_step_lookahead(state, value_table)\n",
    "            value_state_old = np.copy(value_table)\n",
    "            value_table[state] = np.max(q_value)\n",
    "            print(\"After update state:{}\".format(state))\n",
    "            print(value_table.reshape([4, 4]))\n",
    "            process_state_queue.insert(0, state)\n",
    "            already_processed_state.append(state)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    already_processed_state = [final_state]\n",
    "    if convergence_flag(value_state_old, value_table):\n",
    "        break\n",
    "print(\"[Final Value Table] is:\")\n",
    "print(value_table.reshape([4, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.个体不知道终态位置，通过迭代每次对所有状态价值进行更新："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table = np.zeros(16)\n",
    "pi = 0.25\n",
    "gamma = 1.0\n",
    "value_table.reshape([4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -1.]\n",
      " [ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -2.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -3.]\n",
      " [-2. -3. -3. -3.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -4.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Episode 1]:\n",
      "After update state:15\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Final Value Table] is:\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -5.]]\n"
     ]
    }
   ],
   "source": [
    "episode = 1\n",
    "while True:\n",
    "    print(\"[Episode {}]:\".format(episode))\n",
    "    for state in env.state:\n",
    "        q_value = one_step_lookahead(state, value_table)\n",
    "        value_state_old = np.copy(value_table)\n",
    "        value_table[state] = np.max(q_value)\n",
    "    print(\"After update state:{}\".format(state))\n",
    "    print(value_table.reshape([4, 4]))\n",
    "    print(\"\\n\\n\\n\")\n",
    "    if convergence_flag(value_state_old, value_table):\n",
    "        break\n",
    "print(\"[Final Value Table] is:\")\n",
    "print(value_table.reshape([4, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
